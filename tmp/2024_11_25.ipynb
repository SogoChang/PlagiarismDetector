{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfWcEUaxu_6O",
    "outputId": "877dc4a5-2293-4ceb-9877-d491d9e1fb91"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g725pgzAx-ob",
    "outputId": "fd0fdb0e-6b41-4f5e-b1eb-c876b9045fe8"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/AashitaK/Plagiarism-Detection.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZERHDarv_cB",
    "outputId": "42d572cb-7075-4575-bca5-f2facce0c23d"
   },
   "outputs": [],
   "source": [
    "# !pip install nltk pandas scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0DWNffvyZ77",
    "outputId": "034cf3ad-74e4-4ed5-844c-ca7f30ab1e95"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3r8ajaqyuf90",
    "outputId": "28722574-685f-4921-d3fe-68255e37ffbf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk import trigrams, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize\n",
    "\n",
    "# Colab 路徑\n",
    "# path = \"/content/Plagiarism-Detection/input/\"  \n",
    "\n",
    "# 主機路徑\n",
    "path = \"input/\"  \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 檔案清理函數\n",
    "def clean_file(myfile):\n",
    "    text = myfile.read().lower()\n",
    "    text = re.sub(r'[\\n]\\s*|[\\']|[:]|[+]|\\d+|[--]', '', text)\n",
    "    text = re.sub(r'\\(\\)|\\.\\s+\\.', '.', text).strip()\n",
    "    return text\n",
    "\n",
    "# 讀取文件並建立DataFrame\n",
    "def get_dataframe(files):\n",
    "    data = []\n",
    "    for f in files:\n",
    "        with open(path + f, mode='r', encoding='utf-8-sig') as myfile:\n",
    "            data.append(clean_file(myfile))\n",
    "    return pd.DataFrame(data, columns=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eicZR-eluf93"
   },
   "outputs": [],
   "source": [
    "# 載入檔案並建立標註\n",
    "suspicious_files = sorted([f for f in os.listdir(path) if f.startswith('suspicious-document')])\n",
    "suspicious = get_dataframe(suspicious_files)\n",
    "suspicious['File_index'] = [f[19:24] for f in suspicious_files]\n",
    "suspicious['Plagiarized'] = pd.read_csv(path + \"Plagiarized.csv\").Plagiarized\n",
    "\n",
    "source_files = sorted([f for f in os.listdir(path) if f.startswith('source-document')])\n",
    "source = get_dataframe(source_files)\n",
    "source['File_index'] = [f[15:20] for f in source_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIkvgA_ouf94"
   },
   "outputs": [],
   "source": [
    "# 文本處理函數\n",
    "def process_text(df):\n",
    "    punc_stop = set(stopwords.words('english')).union(\n",
    "        {\".\", \",\", \"?\", \"-\", \"!\", \"'\", '\"', \"\\\\\", \"/\", \";\", \"{\", \"}\", \"(\", \")\", \"[\", \"]\", \"''\", \"``\", \"*\", \"$\", \"%\"}\n",
    "    )\n",
    "    df['Tokens'] = df['Text'].apply(word_tokenize).apply(lambda x: [w for w in x if w not in punc_stop])\n",
    "    # 限制三元組的計算至最前面的 500 個詞\n",
    "    df['Trigrams'] = df['Tokens'].apply(lambda x: set(trigrams(x[:500])))\n",
    "    return df\n",
    "\n",
    "# 計算 Jaccard 相似度和 containment 度量\n",
    "def Jaccard_similarity_coefficient(A, B): return len(A.intersection(B)) / len(A.union(B))\n",
    "def containment_measure(A, B): return len(A.intersection(B)) / len(B)\n",
    "\n",
    "def check_plagiarism_Jaccard(doc_trigrams):\n",
    "    return source.Trigrams.apply(lambda s: Jaccard_similarity_coefficient(s, doc_trigrams)).max()\n",
    "\n",
    "def check_plagiarism_containment(doc_trigrams):\n",
    "    return source.Trigrams.apply(lambda s: containment_measure(s, doc_trigrams)).max()\n",
    "\n",
    "# LCS 度量\n",
    "def LCS(A, B):\n",
    "    m, n, longest = len(A), len(B), 0\n",
    "    counter = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if A[i] == B[j]:\n",
    "                count = counter[i][j] + 1\n",
    "                counter[i+1][j+1] = count\n",
    "                longest = max(longest, count)\n",
    "    return longest\n",
    "\n",
    "def check_plagiarism_LCS(doc):\n",
    "    return source.Tokens.apply(lambda s: LCS(s, doc)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JI2HjmpTuf95"
   },
   "outputs": [],
   "source": [
    "suspicious, source = process_text(suspicious), process_text(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBS-wrqKuf95"
   },
   "outputs": [],
   "source": [
    "suspicious['Jaccard_similarity_score'] = suspicious.Trigrams.apply(check_plagiarism_Jaccard)\n",
    "suspicious['Containment_measure_score'] = suspicious.Trigrams.apply(check_plagiarism_containment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6bI-3rouf96"
   },
   "outputs": [],
   "source": [
    "# suspicious['Longest_common_sequence'] = suspicious.Tokens.apply(check_plagiarism_LCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1bk9KYWuf96"
   },
   "outputs": [],
   "source": [
    "# 語義分析（LSA）步驟\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "source.Tokens, suspicious.Tokens = source.Tokens.apply(lambda x: [lemmatizer.lemmatize(w) for w in x]), suspicious.Tokens.apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', token_pattern=None, tokenizer=lambda x: x, preprocessor=lambda x: x, ngram_range=(1, 4), max_features=500)  # 降低max_features\n",
    "combined_tokens = pd.concat([suspicious.Tokens, source.Tokens])\n",
    "DTM = vectorizer.fit_transform(combined_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3jQy1s1uf97"
   },
   "outputs": [],
   "source": [
    "# run LSA\n",
    "LSA = TruncatedSVD(50, algorithm='arpack')  # 降低成分數量\n",
    "DTM_LSA = Normalizer(copy=False).fit_transform(LSA.fit_transform(DTM))\n",
    "\n",
    "# 分塊計算相似度矩陣\n",
    "def compute_similarity_matrix_in_batches(matrix, batch_size=250):\n",
    "    similarity_scores = []\n",
    "    for i in range(0, matrix.shape[0], batch_size):\n",
    "        batch = matrix[i:i + batch_size]\n",
    "        scores = (batch @ matrix.T)\n",
    "        similarity_scores.append(np.max(scores, axis=1))\n",
    "    return np.concatenate(similarity_scores)\n",
    "\n",
    "suspicious['LSA_similarity'] = compute_similarity_matrix_in_batches(DTM_LSA[:len(suspicious)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_MEnMqCuf98",
    "outputId": "50b12c24-d138-4071-952e-2162da5156cf"
   },
   "outputs": [],
   "source": [
    "# # 可視化\n",
    "# sns.swarmplot(x=\"Plagiarized\", y=\"Jaccard_similarity_score\", data=suspicious)\n",
    "# sns.swarmplot(x=\"Plagiarized\", y=\"Containment_measure_score\", data=suspicious)\n",
    "# sns.relplot(x=\"Jaccard_similarity_score\", y=\"Containment_measure_score\", hue=\"Plagiarized\", data=suspicious)\n",
    "\n",
    "# # 相似度特徵與標註的相關性分析\n",
    "# print(suspicious[['LSA_similarity', 'Jaccard_similarity_score', 'Containment_measure_score', 'Plagiarized']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrHq9O5Zuf99",
    "outputId": "84735f73-e7d6-4e4a-b07b-d189fa9f0d80"
   },
   "outputs": [],
   "source": [
    "# # 模型訓練與測試\n",
    "# X, y = suspicious[['LSA_similarity', 'Jaccard_similarity_score', 'Containment_measure_score']], suspicious.Plagiarized\n",
    "# clf = LogisticRegression()\n",
    "\n",
    "# # 使用分層隨機分割的交叉驗證以減少內存需求\n",
    "# sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "# cross_val_scores = cross_val_score(clf, X, y, cv=sss)\n",
    "# print(np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dp6GsjEKuf99",
    "outputId": "429383a3-afd3-42b5-e198-81f6fcb9c95e"
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print(y.value_counts())\n",
    "\n",
    "\n",
    "# from joblib import dump\n",
    "\n",
    "# # Save the trained model to a file\n",
    "\n",
    "# # Colab路徑\n",
    "# # model_path = '/content/drive/MyDrive/Colab Notebooks/logistic_regression_model.joblib'\n",
    "\n",
    "# # Server路徑\n",
    "# model_path = 'logistic_regression_model.joblib'\n",
    "\n",
    "# dump(clf, model_path)\n",
    "# print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9YdxtmAuvci"
   },
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "# Load the saved model\n",
    "model_path = 'logistic_regression_model.joblib'\n",
    "clf = load(model_path)\n",
    "\n",
    "# Now you can use the loaded model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cp8KeRwHHReE",
    "outputId": "c0e3ba29-030a-4017-fd4b-b0d8c0d50304"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump, load\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Model Training and Testing\n",
    "X, y = suspicious[['LSA_similarity', 'Jaccard_similarity_score', 'Containment_measure_score']], suspicious.Plagiarized\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Stratified Shuffle Split\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(y.value_counts())\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_path = 'logistic_regression_model.joblib'\n",
    "dump(clf, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Cross-validation scores\n",
    "cross_val_scores = cross_val_score(clf, X, y, cv=sss)\n",
    "print(f\"Mean cross-validation score: {np.mean(cross_val_scores):.4f}\")\n",
    "\n",
    "# Function to calculate the plagiarism ratio using the loaded model\n",
    "def calculate_plagiarism_ratio_with_model(input_text, model_path):\n",
    "    # Load the trained model\n",
    "    clf = load(model_path)\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "    # Preprocess the input text\n",
    "    tokens = word_tokenize(input_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "    input_text_processed = ' '.join(tokens)\n",
    "\n",
    "    # Calculate TF-IDF and similarity features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    source_texts = [\"Add your reference source texts here\"]  # Replace with actual source texts\n",
    "    source_vectors = vectorizer.fit_transform(source_texts)\n",
    "    input_vector = vectorizer.transform([input_text_processed])\n",
    "\n",
    "    lsa_similarity = cosine_similarity(input_vector, source_vectors).max()\n",
    "    jaccard_similarity = len(set(tokens) & set(' '.join(source_texts).split())) / len(set(tokens) | set(' '.join(source_texts).split()))\n",
    "    containment_measure = len(set(tokens) & set(' '.join(source_texts).split())) / len(set(tokens))\n",
    "\n",
    "    # Create the feature set for prediction\n",
    "    features = [[lsa_similarity, jaccard_similarity, containment_measure]]\n",
    "\n",
    "    # Predict plagiarism\n",
    "    prediction = clf.predict(features)\n",
    "    probability = clf.predict_proba(features).max()\n",
    "\n",
    "    is_plagiarized = True if prediction[0] == 1 else False\n",
    "\n",
    "    # return f\"Plagiarism Ratio: {probability:.2f}, Prediction: {'Plagiarized' if prediction[0] == 1 else 'Not Plagiarized'}\"\n",
    "    return probability, is_plagiarized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the function with user input\n",
    "# while True:\n",
    "#     input_text = input(\"Enter text to check for plagiarism (type 'exit' to quit): \")\n",
    "#     if input_text.lower() == 'exit':\n",
    "#         break\n",
    "#     probability, is_plagiarized = calculate_plagiarism_ratio_with_model(input_text, model_path)\n",
    "#     output = f\"Plagiarism Ratio: {probability:.2f}, Prediction: {'Plagiarized' if is_plagiarized else 'Not Plagiarized'}\"\n",
    "#     print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch gradio\n",
    "import gradio as gr\n",
    "\n",
    "# 定义不同标签页中的功能\n",
    "def Human_Plagiarism_Detection(input_text):\n",
    "    probability, is_plagiarized = calculate_plagiarism_ratio_with_model(input_text, model_path)\n",
    "    output = f\"Plagiarism Ratio: {probability:.2f}, Prediction: {'Plagiarized' if is_plagiarized else 'Not Plagiarized'}\"\n",
    "    return output\n",
    "\n",
    "def AI_Plagiarism_Detection(input_text):\n",
    "    return \"hello\"\n",
    "\n",
    "# 创建界面\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tabs():\n",
    "        # 第一個標籤：Human Plagiarism Detection\n",
    "        with gr.TabItem(\"Human Plagiarism Detection\"):\n",
    "            with gr.Row():\n",
    "                text_input = gr.Textbox(label=\"Input Text\", lines=5, placeholder=\"Enter text here...\")\n",
    "            process_btn = gr.Button(\"Calculate\")\n",
    "            result = gr.Textbox(label=\"Plagiarism Ratio\")\n",
    "            process_btn.click(Human_Plagiarism_Detection, inputs=text_input, outputs=result)\n",
    "\n",
    "\n",
    "        # 第二個標籤：AI Plagiarism Detection\n",
    "        with gr.TabItem(\"AI Plagiarism Detection\"):\n",
    "            with gr.Row():\n",
    "                text_input = gr.Textbox(label=\"Input Text\", lines=5, placeholder=\"Enter text here...\")\n",
    "            process_btn = gr.Button(\"Calculate\")\n",
    "            result = gr.Textbox(label=\"Plagiarism Ratio\")\n",
    "            process_btn.click(Human_Plagiarism_Detection, inputs=text_input, outputs=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
