{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required libraries\n",
    "# !pip install gdown -q\n",
    "# !pip install transformers -q\n",
    "# !pip install seaborn -q\n",
    "\n",
    "# # Download dataset from Google Drive\n",
    "# !gdown --id 1N740x0d6go1RDi_ULrGUy71pM76QGxQU -O dataset.zip\n",
    "\n",
    "# # Unzip the downloaded dataset\n",
    "# !unzip -o dataset.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Paths to the dataset directories\n",
    "# og_arxiv_path = './data/og/arxiv'  # Original (ORIG) texts\n",
    "# mg_arxiv_path = './data/bert-large-cased_parallel_mlm_prob_0.2/mg/arxiv'  # Machine-generated (SPUN) texts\n",
    "\n",
    "og_arxiv_path = 'MachineParaphraseCorpus/og/arxiv'  # Original (ORIG) texts\n",
    "mg_arxiv_path = 'MachineParaphraseCorpus/bert-large-cased_parallel_mlm_prob_0.2/mg/arxiv'  # Machine-generated (SPUN) texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of original and paraphrased files\n",
    "original_files = [f for f in os.listdir(og_arxiv_path) if f.endswith('.txt')]\n",
    "paraphrased_files = [f for f in os.listdir(mg_arxiv_path) if f.endswith('.txt')]\n",
    "\n",
    "# Initialize data list\n",
    "data = []\n",
    "\n",
    "# Match and read files\n",
    "for para_file in paraphrased_files:\n",
    "    # Extract base filename to find corresponding original file\n",
    "    base_filename = para_file.split('-SPUN')[0] + '.txt'  # Adjust the split as per your filenames\n",
    "    original_file = base_filename\n",
    "\n",
    "    if original_file in original_files:\n",
    "        # Read original text\n",
    "        with open(os.path.join(og_arxiv_path, original_file), 'r', encoding='utf-8') as f:\n",
    "            original_text = f.read()\n",
    "\n",
    "        # Read paraphrased text\n",
    "        with open(os.path.join(mg_arxiv_path, para_file), 'r', encoding='utf-8') as f:\n",
    "            paraphrased_text = f.read()\n",
    "\n",
    "        # Append positive example\n",
    "        data.append({'original': original_text, 'paraphrased': paraphrased_text, 'label': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match and create positive examples\n",
    "for para_file in paraphrased_files:\n",
    "    # Extract the prefix and number from the paraphrased file\n",
    "    prefix = para_file.split('-SPUN')[0]  # Extract '0704.0097'\n",
    "    number = para_file.split('-SPUN-')[-1].split('.')[0]  # Extract '3'\n",
    "\n",
    "    # Construct the corresponding original filename\n",
    "    original_file = f\"{prefix}-ORIG-{number}.txt\"\n",
    "\n",
    "    # Check if the corresponding original file exists\n",
    "    if original_file in original_files:\n",
    "        # Read the original text\n",
    "        with open(os.path.join(og_arxiv_path, original_file), 'r', encoding='utf-8') as f:\n",
    "            original_text = f.read()\n",
    "\n",
    "        # Read the paraphrased text\n",
    "        with open(os.path.join(mg_arxiv_path, para_file), 'r', encoding='utf-8') as f:\n",
    "            paraphrased_text = f.read()\n",
    "\n",
    "        # Append the positive example\n",
    "        data.append({'original': original_text, 'paraphrased': paraphrased_text, 'label': 1})\n",
    "\n",
    "# Combine positive and negative samples\n",
    "full_data = data + negative_data\n",
    "df = pd.DataFrame(full_data)\n",
    "print(f\"Total dataset size: {len(df)}\")\n",
    "print(df['label'].value_counts())  # Check label distribution\n",
    "\n",
    "# Prepare data for training\n",
    "texts = list(zip(df['original'], df['paraphrased']))\n",
    "labels = df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Load Pretrained Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Tokenize Text\n",
    "train_encodings = tokenizer(\n",
    "    [t[0] for t in train_texts], [t[1] for t in train_texts], truncation=True, padding=True)\n",
    "val_encodings = tokenizer(\n",
    "    [t[0] for t in val_texts], [t[1] for t in val_texts], truncation=True, padding=True)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ParaphraseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.tolist()  # Ensure labels are in list format\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ParaphraseDataset(train_encodings, train_labels)\n",
    "val_dataset = ParaphraseDataset(val_encodings, val_labels)\n",
    "\n",
    "# # Training Arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     per_device_train_batch_size=8,  # Adjust based on available GPU memory\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=10,\n",
    "#     save_strategy='no'\n",
    "# )\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=1,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy='no'\n",
    ")\n",
    "\n",
    "# Compute Metrics Function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Save Model in PyTorch\n",
    "def save_model_pytorch(model, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model_path = os.path.join(output_dir, \"model.pth\")\n",
    "    print(f\"Saving model to {model_path}...\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "# Load Model in PyTorch\n",
    "def load_model_pytorch(model_class, model_path, config):\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    model = model_class.from_pretrained(None, config=config)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Generate Predictions\n",
    "predictions = trainer.predict(val_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Original', 'Paraphrased'],\n",
    "            yticklabels=['Original', 'Paraphrased'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, pred_labels, target_names=['Original', 'Paraphrased']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
