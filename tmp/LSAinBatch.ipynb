{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/undergrad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/undergrad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/undergrad/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk import trigrams, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize\n",
    "path = \"input/\"  # 設定資料路徑\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 檔案清理函數\n",
    "def clean_file(myfile):\n",
    "    text = myfile.read().lower()\n",
    "    text = re.sub(r'[\\n]\\s*|[\\']|[:]|[+]|\\d+|[--]', '', text)\n",
    "    text = re.sub(r'\\(\\)|\\.\\s+\\.', '.', text).strip()\n",
    "    return text\n",
    "\n",
    "# 讀取文件並建立DataFrame\n",
    "def get_dataframe(files):\n",
    "    data = []\n",
    "    for f in files:\n",
    "        with open(path + f, mode='r', encoding='utf-8-sig') as myfile:\n",
    "            data.append(clean_file(myfile))\n",
    "    return pd.DataFrame(data, columns=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入檔案並建立標註\n",
    "suspicious_files = sorted([f for f in os.listdir(path) if f.startswith('suspicious-document')])\n",
    "suspicious = get_dataframe(suspicious_files)\n",
    "suspicious['File_index'] = [f[19:24] for f in suspicious_files]\n",
    "suspicious['Plagiarized'] = pd.read_csv(path + \"Plagiarized.csv\").Plagiarized\n",
    "\n",
    "source_files = sorted([f for f in os.listdir(path) if f.startswith('source-document')])\n",
    "source = get_dataframe(source_files)\n",
    "source['File_index'] = [f[15:20] for f in source_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本處理函數\n",
    "def process_text(df):\n",
    "    punc_stop = set(stopwords.words('english')).union(\n",
    "        {\".\", \",\", \"?\", \"-\", \"!\", \"'\", '\"', \"\\\\\", \"/\", \";\", \"{\", \"}\", \"(\", \")\", \"[\", \"]\", \"''\", \"``\", \"*\", \"$\", \"%\"}\n",
    "    )\n",
    "    df['Tokens'] = df['Text'].apply(word_tokenize).apply(lambda x: [w for w in x if w not in punc_stop])\n",
    "    # 限制三元組的計算至最前面的 500 個詞\n",
    "    df['Trigrams'] = df['Tokens'].apply(lambda x: set(trigrams(x[:500])))\n",
    "    return df\n",
    "\n",
    "# 計算 Jaccard 相似度和 containment 度量\n",
    "def Jaccard_similarity_coefficient(A, B): return len(A.intersection(B)) / len(A.union(B))\n",
    "def containment_measure(A, B): return len(A.intersection(B)) / len(B)\n",
    "\n",
    "def check_plagiarism_Jaccard(doc_trigrams): \n",
    "    return source.Trigrams.apply(lambda s: Jaccard_similarity_coefficient(s, doc_trigrams)).max()\n",
    "\n",
    "def check_plagiarism_containment(doc_trigrams): \n",
    "    return source.Trigrams.apply(lambda s: containment_measure(s, doc_trigrams)).max()\n",
    "\n",
    "# LCS 度量\n",
    "def LCS(A, B):\n",
    "    m, n, longest = len(A), len(B), 0\n",
    "    counter = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if A[i] == B[j]:\n",
    "                count = counter[i][j] + 1\n",
    "                counter[i+1][j+1] = count\n",
    "                longest = max(longest, count)\n",
    "    return longest\n",
    "\n",
    "def check_plagiarism_LCS(doc): \n",
    "    return source.Tokens.apply(lambda s: LCS(s, doc)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious, source = process_text(suspicious), process_text(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious['Jaccard_similarity_score'] = suspicious.Trigrams.apply(check_plagiarism_Jaccard)\n",
    "suspicious['Containment_measure_score'] = suspicious.Trigrams.apply(check_plagiarism_containment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious['Longest_common_sequence'] = suspicious.Tokens.apply(check_plagiarism_LCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語義分析（LSA）步驟\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "source.Tokens, suspicious.Tokens = source.Tokens.apply(lambda x: [lemmatizer.lemmatize(w) for w in x]), suspicious.Tokens.apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', token_pattern=None, tokenizer=lambda x: x, preprocessor=lambda x: x, ngram_range=(1, 4), max_features=500)  # 降低max_features\n",
    "combined_tokens = pd.concat([suspicious.Tokens, source.Tokens])\n",
    "DTM = vectorizer.fit_transform(combined_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSA = TruncatedSVD(50, algorithm='arpack')  # 降低成分數量\n",
    "DTM_LSA = Normalizer(copy=False).fit_transform(LSA.fit_transform(DTM))\n",
    "\n",
    "# 分塊計算相似度矩陣\n",
    "def compute_similarity_matrix_in_batches(matrix, batch_size=250):\n",
    "    similarity_scores = []\n",
    "    for i in range(0, matrix.shape[0], batch_size):\n",
    "        batch = matrix[i:i + batch_size]\n",
    "        scores = (batch @ matrix.T)\n",
    "        similarity_scores.append(np.max(scores, axis=1))\n",
    "    return np.concatenate(similarity_scores)\n",
    "\n",
    "suspicious['LSA_similarity'] = compute_similarity_matrix_in_batches(DTM_LSA[:len(suspicious)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Plagiarized', ylabel='Jaccard_similarity_score'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可視化\n",
    "sns.swarmplot(x=\"Plagiarized\", y=\"Jaccard_similarity_score\", data=suspicious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Plagiarized', ylabel='Jaccard_similarity_score'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.swarmplot(x=\"Plagiarized\", y=\"Containment_measure_score\", data=suspicious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7cf7cd21a960>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.relplot(x=\"Jaccard_similarity_score\", y=\"Containment_measure_score\", hue=\"Plagiarized\", data=suspicious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           LSA_similarity  Jaccard_similarity_score  \\\n",
      "LSA_similarity                   1.000000                 -0.041913   \n",
      "Jaccard_similarity_score        -0.041913                  1.000000   \n",
      "Containment_measure_score       -0.043257                  0.995360   \n",
      "Plagiarized                     -0.082941                  0.146659   \n",
      "\n",
      "                           Containment_measure_score  Plagiarized  \n",
      "LSA_similarity                             -0.043257    -0.082941  \n",
      "Jaccard_similarity_score                    0.995360     0.146659  \n",
      "Containment_measure_score                   1.000000     0.148125  \n",
      "Plagiarized                                 0.148125     1.000000  \n"
     ]
    }
   ],
   "source": [
    "# 相似度特徵與標註的相關性分析\n",
    "print(suspicious[['LSA_similarity', 'Jaccard_similarity_score', 'Containment_measure_score', 'Plagiarized']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62\n"
     ]
    }
   ],
   "source": [
    "# 模型訓練與測試\n",
    "X, y = suspicious[['LSA_similarity', 'Jaccard_similarity_score', 'Containment_measure_score']], suspicious.Plagiarized\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# 使用分層隨機分割的交叉驗證以減少內存需求\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "cross_val_scores = cross_val_score(clf, X, y, cv=sss)\n",
    "print(np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.77        62\n",
      "           1       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.62       100\n",
      "   macro avg       0.31      0.50      0.38       100\n",
      "weighted avg       0.38      0.62      0.47       100\n",
      "\n",
      "Plagiarized\n",
      "0    311\n",
      "1    189\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
